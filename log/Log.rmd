---
author: Lisa Hu
output:
  pdf_document:
    includes:
      before_body: title.sty
    keep_tex : true
    toc: true
    toc_depth: 2
    number_sections: true
    latex_engine: xelatex
---
```{r include = FALSE}
# Copyright (c) 2022 Lisa Hu
# Licensed under GPLv3. See LICENSE
```

```{r setup, include = FALSE, warning = FALSE, message = FALSE, results = "hide"}
#' Setup chunk
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::read_chunk("../src/log.R")
knitr::read_chunk("../src/data_cleaning.R")
source("../src/functions.R", local = knitr::knit_global())
```
[//]: # (toc)

\newpage
```{r libraries, warning = FALSE, message = FALSE, include = FALSE}
#' Load all the packages
packages <- c("dplyr", "ggplot2", "readr", "ggpubr", "pander", "ggbiplot", "FSA")
invisible(lapply(packages, library, character.only = T))
```

# Data description
The data can be found on kaggle.com: [Urinary biomarkers for pancreatic cancer](https://www.kaggle.com/datasets/johnjdavisiv/urinary-biomarkers-for-pancreatic-cancer)
The files are saved as `~/data/Data.csv` and `~/data/Documentation.csv` for easier access.

The following packages were used:

+ dplyr
+ readr
+ pander
+ FSA
+ ggplot2
+ ggpubr
+ ggbiplot

\newpage
# Reading the data
We first want to create an insight of our data:
```{r read-data}
```

The information given in the codebook originates from the `~/data/Documentation.csv`. This file was given with the data file and can be found on the website.

\newpage
# Manipulate the data
A lot of the rows contain empty strings instead of NA, which has to be fixed first. Besides that, the columns `sample_id` , `patient_cohort`, `sample_origin`, and `benign_sample_diagnosis` in the dataset significant value for the analysis and are therefor dropped. A column `diagnosis_group` was added for a comparison test.

```{r change-data}
```

\newpage
## REG1A vs. REG1B
```{r reg-vs}
```

Although performance between the two is similar, a Kruskal-Wallis test with Dunn's multiple comparisons shows that REG1B outperforms REG1A when the control and benign samples are compared to the I-IIA PDAC samples.
Therefor, REG1B was used further on in the experiments and REG1A is dropped.

## Log transformation
A summary of the data shows very high maximum values, but rather low medians.
A log-transformation is applied to correct this.

```{r log-trans}
```

The samples are then grouped by diagnosis for easier access of the different samples.
Table 5 shows the different amounts of samples per diagnosis and the amount of which are also blood samples.
\newpage
```{r demograph}
```

\newpage
# Analyse the data
## Boxplots
```{r exp-box, fig.height = 8}
```

The outliers are not localized in a specific diagnosis group, but rather spread over the groups.

\newpage

## Correlation matrix
```{r correlation}
```

The heatmap shows that there is not much correlation between creatinine and the other variables.
The other outstanding one has to be the TFF1 biomarker, being the most correlated variable to others.

\newpage

## PCA
```{r pca}
```

While the control and benign group show relative distance from the PDAC group, there is a still a lot of overlapping samples with the benign and PDAC groups.
As earlier concluded from the heatmap, the creatinine biomarker does not show much relativeness with the other biomarkers.
LYVE1 is nicely in between the TFF1 and REG1B biomarkers.
Every point close tho the origin have values close to the mean for all variables.

\newpage
# Machine Learning
## Quality Metrics
Accuracy is the most important quality metric to measure the performance of an algorithm. Though it is easy to choose an algorithm this way, there are always multiple algorithms one can take as final option. Hence, other quality metrics have to be taken in account to make the optimal choice.

For this project, the model is a finished product, trained and tested with the already collected data. New samples for this model are manually inserted by the acting physician, thus speed is not a relevant metric. Naturally, it is of more importance a patient with a malignant cancer should not be classified as benign, rather than a patient with a benign case being classified as malignant. These errors can be visualized in a confusion matrix, which almost all algorithms output.

### Confusion Matrix
A standard confusion matrix is a 2x2 matrix which shows all the correct hits and rejections, and errors. In Weka, a confusion matrix looks a bit like this:
```{=latex}
\begin{table}[h]
    \centering
    \caption{Example of a confusion matrix}
    \label{tab:ex_cm}
    \begin{tabular}{cc|l}
        a & b & <- classified as\\
        \hline
        TP & FN & a = Control/Benign (healty)\\
        FP & TN & b = Malignant\\
    \end{tabular}
\end{table}
```

In this example, the correctly classified "healthy" instances are true positive (TP) and correctly classified "Malignant" instances are true negative (TN). The "healthy" instances that were classified as "Malignant" are false positive (FP) and malignant instances that were classified as benign are false negative (FN).

### Sensitivity and False Positive Rate
Generally important for machine learning algorithms, but also for this project: Sensitivity and False Positive Rate (FPR). Also known as the true positive rate (TPR), sensitivity is calculated as $\frac{TP}{TP+FN}$. The FPR is calculated as $\frac{FP}{FP+TN}$. The FPR can also be derived from the True Negative Rate (TNR): $FPR = 1 - TNR$

These two

\newpage
## Weka: Model exploration
For the exploration of the model, the data is cleaned it contains only the biomarkers and the classification labels (Control, Benign or PDAC).
```{r cleaning, eval = FALSE}
```

The `~/data/wekafiles/base.exp` file contains all the options used for a baseline run in Weka. The data is run through different types of algorithms with 10-fold cross validation:
```{r weka-base}
```

These results show a relative low accuracy and sensitivity. Some algorithms also have a low ROC value, putting the cutoff at 0.8: OneR, SMO, IBk and J48 will not be used. As for the remaining three: NaiveBayes has by far the lowest accuracy of them and is therefor also dropped. Leaving the options Logistic and RandomForest. Since earlier shown there is a linear correlation between the different variables, the Logistic algorithm would be more fitting for this type of data.

\newpage
### Data imbalance
To prepare the data for the optimization, the data needs to be split into groups: one file containing `Control` and `PDAC` samples, another file containing `Benign` and `PDAC` samples:
```{r splitting, eval = FALSE}
```

### Algorithm optimization
For the optimization of the algorithm, Weka's ThresholdSelector classifier will be used. This algorithm allows a threshold on the probability output of the given classifier. It is important that the FPR is as low as possible, since no patient wants to be diagnosed healthy when there is something serious swarming around. Therefor, the ThresholdSelector's designated class is set to `second class value`, since the "PDAC" instances are after the healthy instances. The measure to set the threshold is TPR.  Again, these options can be imported from `~/data/wekafiles/optimization.exp`.
```{r optimization}
```

The results of the `Logistic` classifier shows the base algorithm without the optimization. Though the accuracy is high, the FPR is too. The FPR needs to be as low as possible without the expense of the accuracy and TPR, so the highest TPR and lowest FPR possible. Using the `Analyse` tab in the Weka Experimenter, Paired T-Tests can be performed to see which algorithm is significant different on the given comparison field. Comparing with a significance of 0.05, the best option for the Control model is the one with a cutoff of 0.600, whereas the Benign model would be best with a cutoff of 0.55. These two highest cutoffs that show no significant raise in FPR when compared to the Logistic algorithm.
