---
author: Lisa Hu
output:
  pdf_document:
    includes:
      before_body: title.sty
    keep_tex : true
    toc: true
    toc_depth: 2
    number_sections: true
    latex_engine: xelatex
---
```{r include = FALSE}
# Copyright (c) 2022 Lisa Hu
# Licensed under GPLv3. See LICENSE
```

```{r setup, include = FALSE, warning = FALSE, message = FALSE, results = "hide"}
#' Setup chunk
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::read_chunk("EDA.R")
source("../src/functions.R", local = knitr::knit_global())
```
[//]: # (toc)

\newpage
```{r libraries, warning = FALSE, message = FALSE, include = FALSE}
#' Load all the packages
packages <- c("dplyr", "ggplot2", "readr", "ggpubr", "pander", "ggbiplot", "FSA")
invisible(lapply(packages, library, character.only = T))
```

# Data description
The data can be found on kaggle.com: [Urinary biomarkers for pancreatic cancer](https://www.kaggle.com/datasets/johnjdavisiv/urinary-biomarkers-for-pancreatic-cancer)
The files are saved as `Data.csv` and `Documentation.csv` for easier access.

The following packages were used:

+ dplyr
+ readr
+ pander
+ FSA
+ ggplot2
+ ggpubr
+ ggbiplot

\newpage
# Reading the data
We first want to create an insight of our data:
```{r read-data}
```

The information given in the codebook originates from the `Documentation.csv`. This file was given with the data file and can be found on the website.

\newpage
# Manipulate the data
A lot of the rows contain empty strings instead of NA, which has to be fixed first. Besides that, the columns `sample_id` , `patient_cohort`, `sample_origin`, and `benign_sample_diagnosis` in the dataset significant value for the analysis and are therefor dropped. A column `diagnosis_group` was added for a comparison test.

```{r change-data}
```

\newpage
## REG1A vs. REG1B
```{r reg-vs}
```

Although performance between the two is similar, a Kruskal-Wallis test with Dunn's multiple comparisons shows that REG1B outperforms REG1A when the control and benign samples are compared to the I-IIA PDAC samples.
Therefor, REG1B was used further on in the experiments and REG1A is dropped.

## Log transformation
A summary of the data shows very high maximum values, but rather low medians.
A log-transformation is applied to correct this.

```{r log-trans}
```

The samples are then grouped by diagnosis for easier access of the different samples.
Table 5 shows the different amounts of samples per diagnosis and the amount of which are also blood samples.
\newpage
```{r demograph}
```

\newpage
# Analyse the data
## Boxplots
```{r exp-box, fig.height = 8}
```

The outliers are not localized in a specific diagnosis group, but rather spread over the groups.

\newpage

## Correlation matrix
```{r correlation}
```

The heatmap shows that there is not much correlation between creatinine and the other variables.
The other outstanding one has to be the TFF1 biomarker, being the most correlated variable to others.

\newpage

## PCA
```{r pca}
```

While the control and benign group show relative distance from the PDAC group, there is a still a lot of overlapping samples with the benign and PDAC groups.
As earlier concluded from the heatmap, the creatinine biomarker does not show much relativeness with the other biomarkers.
LYVE1 is nicely in between the TFF1 and REG1B biomarkers.
Every point close tho the origin have values close to the mean for all variables.

\newpage
# Machine Learning
## Quality Metrics
Accuracy is the most important quality metric to measure the performance of an algorithm. Though it is easy to choose an algorithm this way, there are always multiple algorithms one can take as final option. Hence, other quality metrics have to be taken in account to make the optimal choice.

For this project, the model is a finished product, trained and tested with the already collected data. New samples for this model are manually inserted by the acting physician, thus speed is not a relevant metric. Naturally, it is of more importance a patient with a malignant cancer should not be classified as benign, rather than a patient with a benign case being classified as malignant. These errors can be visualized in a confusion matrix, which almost all algorithms output.

### Confusion Matrix
A standard confusion matrix is a 2x2 matrix which shows all the correct hits and rejections, and errors. In Weka, a confusion matrix looks a bit like this:
```{=latex}
\begin{table}[h]
    \centering
    \caption{Example of a confusion matrix}
    \label{tab:ex_cm}
    \begin{tabular}{cc|l}
        a & b & <- classified as\\
        \hline
        TP & FP & a = Malignant\\
        FN & TN & b = Benign\\
    \end{tabular}
\end{table}
```

In this example, the correctly classified malignant instances are true positive (TP) and correctly classified benign instances are true negative (TN). The benign instances that were classified as malignant are false positive (FP) and malignant instances that were classified as benign are false negative (FN).

The example shows a situation where the algorithm can choose between two classes. In this project, there are 4 different classes, so what does a confusion matrix like that look like?Since there is no one TP or one FN, the different values are given per class. To get the confusion matrix of one class, the chosen class is the hit instance (positive), whereas all the other classes are the rejections (negatives).\newline
The confusion matrices for this project will look a bit like this:
```{=latex}
\begin{table}[h]
    \centering
    \caption{Example of a project confusion matrix}
    \label{tab:cm_project}
    \begin{tabular}{cccc|l}
        a & b & c & d & <- classified as\\
        \hline
        hit & error & error & error & a = Control\\
        error & hit & error & error & b = Benign\\
        error & error & hit & error & c = I-II\\
        error & error & error & hit & d = III-IV\\
    \end{tabular}
\end{table}
```

### Sensitivity and specificity
Generally important for machine learning algorithms, but also for this project: Sensitivity and specificity. Also known as the true positive rate (TPR), sensitivity is calculated as $\frac{TP}{TP+FN}$ whereas specificity - or the true negative rate (TNR) - is calculated as $\frac{TN}{TN+FP}$.

### Area under ROC
A receiver operating characteristic curve, or ROC curve, is a curve when the TPR is plotted against the false positive rate (FPR). The curve describes how well the algorithm classified and can be manipulated with various cutoffs.

why important metric

\newline
## Weka: Model exploration
For the exploration of the model, the data is cleaned it contains only the biomarkers and the classification labels (Control, Benign, I-II, and III-IV). For the code used to prepare the data, see Section \ref{appendix-a}.
```{r}
cleaned <- read.csv("../data/cleaned_data.csv")
pander(head(cleaned))
```

To set a baseline, the data is run through different types of algorithms in Weka with 10-fold cross validation:
```{r weka-base}
```

With the `data/base.exp` file, all the options used for this run can be imported into Weka. Add the desired data and fill in the `Results Destination` for the run.

These results show a relative low accuracy and sensitivity. Some algorithms also have a low ROC value, putting the cutoff at 0.8: OneR, SMO, IBk and J48 will not be used. As for the remaining three: NaiveBayes has by far the lowest accuracy of them and is therefor also dropped. Leaving the options SimpleLogistic and RandomForest.

### Optimize algorithm
To create the optimal model, the data is first split into training and test data in a 70/30 ratio (see Section \ref{appendix-a}). To make sure the algorithm learns to predict less

\newpage
# Appendix A
```{r file='../data/data_cleaning.R', eval = FALSE, attr.source='.numberLines'}
```
